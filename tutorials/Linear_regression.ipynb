{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSOR BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is a multidimensional array. A CS-tensor can be used as a function that takes in multiple vectors and can outputs either other vectors, tensors or scalars. The tensors in these neural nets are the things that keep track of what has been learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch ## import module\n",
    "\n",
    "tensor_empty = torch.empty(3,3) #parameter is size and dimensions, here it is 2D vector with 3 items each\n",
    "\n",
    "tensor_random = torch.rand(1,2,3)# prints a random torch\n",
    "\n",
    "tensor_zero = torch.zeros() # gives 0s for all items, can use ones instead etc\n",
    "\n",
    "x = torch.ones(2,2, dtype=torch.float16) # specify class to float 16\n",
    "\n",
    "x = torch.tensor([2.5,0.1]) # create data from a python list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform calculation operations with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "z = x + y #or z = torch.add(x,y)\n",
    "print(z)\n",
    "\n",
    "z = torch.sub(x,y) # substraction\n",
    "z = torch.mul(x,y) # multiplication\n",
    "\n",
    "y.mul_(x) # multiplies all items inside () to y. called trailing underline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform slicing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:,0]) #only first column as rows\n",
    "print(x[1,:]) #prints row 1, all columns\n",
    "print(x[1,1]) #prints element 1:1, you can use print(x[1,1]).item() to get the actual value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reshape tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,4) # gives 16 values\n",
    "print(x)\n",
    "y = x.view(16) # prints 16 values in one dimension, you could also do y = x.view(-1,8)\n",
    "# if you put -1, pytorch will fix the size in this case x.view(2,8)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting from torch tensor to numpy array\n",
    "note: if tensor is on CPU and not GPU, then both objects share same memory allocation. Changing one changes the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(type(b))\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) #added +1 to b too because they both point to same memory location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting from  numpy array to torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "\n",
    "a += 1\n",
    "print(a)\n",
    "print(b) # tensor gets modified too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if operations are being conducted on CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): #checks if cupa is installed\n",
    "    \n",
    "device = torch.device(\"cuda\")\n",
    "x = torch.ones(5, device = device)\n",
    "y = torch.ones(5)\n",
    "y = y.to(device)\n",
    "z = x + y\n",
    "z = z.to(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells pytorch that it will need to calculate the gradiant of the tensor later in the optimization steps. When you have a variable you want to optimize, you need to specify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5, requires_grad=True)# by default it is false\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIANT CALCULATIONS WITH AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd: This class is an engine to calculate derivatives. It records a graph of all the operations performed on a gradient enabled tensor and creates an acyclic graph called the dynamic computational graph. The leaves of this graph are input tensors and the roots are output tensors. Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule.\n",
    "\n",
    "Neural networks are nothing more than composite mathematical functions that are delicately tweaked (trained) to output the required result. The tweaking or the training is done through a remarkable algorithm called backpropagation. Backpropagation is used to calculate the gradients of the loss with respect to the input weights to later update the weights and eventually reduce the loss. i.e. it is basically the application of the calculus chain rule.\n",
    "\n",
    "Creating and training a neural network involves the following essential steps:\n",
    "1. Define the architecture\n",
    "2. Forward propagate on the architecture using input data\n",
    "3. Calculate the loss\n",
    "4. Backpropagate to calculate the gradient for each weight\n",
    "5. Update the weights using a learning rate\n",
    "\n",
    "The change in the loss for a small change in an input weight is called the gradient of that weight and is calculated using backpropagation. The gradient is then used to update the weight using a learning rate to overall reduce the loss and train the neural net.\n",
    "This is done in an iterative way. For each iteration, several gradients are calculated and something called a computation graph is built for storing these gradient functions. PyTorch does it by building a Dynamic Computational Graph (DCG). This graph is built from scratch in every iteration providing maximum flexibility to gradient calculation. For example, for a forward operation (function)Mul a backward operation (function) called MulBackwardis dynamically integrated in the backward graph for computing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# calculate gradiant of function with respect to x\n",
    "y = x+2 # creates computational graph, the operation is the +, input are x and 2 and output is y.\n",
    "\n",
    "print(y) # its at addbackward, it is an addition\n",
    "\n",
    "z = y*y*2\n",
    "print(z)\n",
    "\n",
    "z = z.mean()# operation is meanBackward\n",
    "z.backward() # calculates gradiant z with respect to x (dz/dx), no need for argument since it is scalar\n",
    "\n",
    "print(\"gradiants are\", x.grad) # calculates gradiants using partial derivates x radian vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the last operation creates a scalar value (#2), then we do not need to pass an argument into z.backward. If it does not create a scalar value, we would need to pass it in as an argument (#1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 \n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float)\n",
    "z.backward(v)\n",
    "\n",
    "#2 \n",
    "z = z.mean()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes during training loop, when wanting to update the weights (shouldn't be part of the gradiant computation).  We have three options to prevent this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#1: x.requires_grad_(False)\n",
    "x.requires_grad_(False)\n",
    "\n",
    "#2: x.detach() \n",
    "y = x.detach() # creates new tensor that doesn't require the gradiant\n",
    "\n",
    "#3: wrap it with a with torch.no_grad():\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y) # no gradiant function here (addbackward0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: whenever we call the backward function, the gradiant for the tensor is acumulated in the .grad attribute. the value are summed up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "#training loop\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum() # simulates a model output\n",
    "    \n",
    "    model_output.backward() # calculates the gradiants\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    # all values are summed up, which we don't want, we must empty the gradiants before moving on to \n",
    "    # optimization step\n",
    "    weights.grad.zero_()Summary: it is important to remember when you are trying to calculate gradiants to specify the requires_grad parameter, if it is set to True you can calculate the gradiant using the .backward() function.\n",
    "Before doing the next iteration in the optimization step we need to empty the gradiants with one of the three methods mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important in the training step, useful in the pytorch optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01) # stochastic gradiant descent \n",
    "optimizer.step()\n",
    "optimizer.zero_grad() # does the same as [19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: it is important to remember when you are trying to calculate gradiants to specify the requires_grad parameter, if it is set to True you can calculate the gradiant using the .backward() function.\n",
    "Before doing the next iteration in the optimization step we need to empty the gradiants with one of the three methods mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKPROPAGATION: THEORY WITH EXAMPLE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
